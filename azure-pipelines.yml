trigger:
  - main
  - feature/*

variables:
  - name: isMain
    value: $[eq(variables['Build.SourceBranch'], 'refs/heads/main')]
  - name: isDev
    value: $[eq(variables['Build.SourceBranch'], 'refs/heads/dev')]
  - name: isRelease
    value: $[contains(variables['Build.SourceBranch'], 'refs/heads/release')]
  - name: isBuild
    value: $[or(or(variables.isMain, variables.isDev), variables.isRelease)]
  - name: DatabricksWorkspace
    # TODO - add your databricks workspace url here
    value: https://adb-7622639135174415.15.azuredatabricks.net
  - name: solutionPath
    # TODO - add your DataM8 solutionfile name here
    value: $(System.DefaultWorkingDirectory)/ORAYLISDatabricksSample.dm8s

stages:
  - stage: build
    displayName: Build Solution
    jobs:
      - job: installer
        displayName: Build Solution
        pool:
          vmImage: ubuntu-latest
        steps:
          - task: DownloadGitHubRelease@0
            displayName: Download latest ORAYLIS DataM8 release from github
            inputs:
              connection: github.com_ljenzen
              userRepository: oraylis/automation
              defaultVersionType: "latest" # 'latest' | 'specificVersion' | 'specificTag'. Required. Default version. Default: latest.
              downloadPath: "$(System.DefaultWorkingDirectory)/DataM8" # string. Required. Destination directory. Default: $(System.ArtifactsDirectory).
          - task: Bash@3
            displayName: Install Dm8Data (Generator) via Python
            inputs:
              targetType: "inline"
              script: |
                file="$(ls -1 $(System.DefaultWorkingDirectory)/DataM8/ | grep .whl)"
                command="pip install $(System.DefaultWorkingDirectory)/DataM8/$file"
                eval $command
          - task: ExtractFiles@1
            displayName: Extract DataM8Validate
            inputs:
              archiveFilePatterns: "$(System.DefaultWorkingDirectory)/DataM8/DataM8Validate.linux-x64.zip"
              destinationFolder: "$(System.DefaultWorkingDirectory)/DataM8/DataM8Validate"
              cleanDestinationFolder: true
              overwriteExistingFiles: false
          - task: Bash@3
            displayName: Trigger Validator
            inputs:
              targetType: "inline"
              script: |
                chmod a+x ./Dm8Validate
                ./Dm8Validate -f $(solutionPath)
              workingDirectory: "$(System.DefaultWorkingDirectory)/DataM8/DataM8Validate/"
          - task: Bash@3
            displayName: Trigger Dm8Data (Generator)
            inputs:
              targetType: "inline"
              script: >-
                Dm8Data
                -a generate_template
                -s "$(solutionPath)"
                -src ./Generate/databricks-lake
                -dest ./Output
                -m ./Generate/databricks-lake/__modules
                -c ./Generate/databricks-lake/__collections
          - task: CopyFiles@2
            displayName: Copy Output to binaries
            inputs:
              SourceFolder: $(System.DefaultWorkingDirectory)/Output
              Contents: |
                **
              TargetFolder: $(Build.BinariesDirectory)
          - task: PublishBuildArtifacts@1
            displayName: "Publish Artifact: generate"
            inputs:
              PathtoPublish: $(Build.BinariesDirectory)
              ArtifactName: generate
              publishLocation: Container
          # test generated artifacts
          - task: Bash@3
            displayName: Install Ruff
            inputs:
              targetType: "inline"
              script: >
                pip install ruff
          - task: Bash@3
            displayName: Ruff Check Output
            inputs:
              workingDirectory: $(System.DefaultWorkingDirectory)/Output
              targetType: "inline"
              script: >
                ruff check
                --output-format junit
                -o $(Common.TestResultsDirectory)/python-linting.xml
                --no-respect-gitignore
          - task: PublishTestResults@2
            displayName: Publish Test Results
            condition: always()
            inputs:
              testResultsFormat: JUnit
              testResultsFiles: $(Common.TestResultsDirectory)/**.xml

  - stage: deploydev
    displayName: Deploy to dev
    pool:
      vmImage: ubuntu-latest
    condition: and(succeeded(), eq(variables.isMain, true))
    dependsOn: build
    jobs:
      - deployment: deploydev
        displayName: Deploy to Databricks
        environment: dev_databricks
        strategy:
          runOnce:
            deploy:
              steps:
                - checkout: none
                - task: Bash@3
                  displayName: Install Databricks CLI
                  inputs:
                    targetType: "inline"
                    script: |
                      curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
                - task: AzureCLI@2
                  displayName: Azure Login and Deploy Databricks Asset Bundle
                  inputs:
                    azureSubscription: "automation-dev"
                    scriptType: "bash"
                    scriptLocation: "inlineScript"
                    inlineScript: |
                      cat > ~/.databrickscfg << EOF
                      [databricksconf]
                      host = $(DatabricksWorkspace)
                      EOF

                      databricks bundle validate
                      databricks bundle deploy -t dev -p databricksconf
                    addSpnToEnvironment: true
                    useGlobalConfig: true
                    workingDirectory: "$(Agent.BuildDirectory)/generate/"
                - task: AzureCLI@2
                  displayName: Run integration tests
                  inputs:
                    azureSubscription: automation-dev
                    scriptType: bash
                    scriptLocation: inlineScript
                    addSpnToEnvironment: true
                    useGlobalConfig: true
                    workingDirectory: '$(Agent.BuildDirectory)/generate/'
                    inlineScript: |
                      cat > ~/.databrickscfg << EOF
                      [databricksconf]
                      host = $(DatabricksWorkspace)
                      EOF

                      echo "##[group]Run $CREATE_JOB_KEY job"

                      databricks bundle run -t $TARGET -p databricksconf --restart $CREATE_JOB_KEY

                      EXIT_CODE=$?

                      if [[ $EXIT_CODE != 0 ]]; then
                        echo "##vso[task.logissue type=error]Failed running job $CREATE_JOB_KEY"
                        exit $EXIT_CODE
                      fi

                      echo "##[endgroup]"

                      echo "##[group]Run $LOAD_JOB_KEY job"

                      databricks bundle run -t $TARGET -p databricksconf --restart $LOAD_JOB_KEY

                      EXIT_CODE=$?

                      if [[ $EXIT_CODE != 0 ]]; then
                        echo "##vso[task.logissue type=error]Failed running job $LOAD_JOB_KEY"
                        exit $EXIT_CODE
                      fi

                      echo "##[endgroup]"
                  env:
                    CREATE_JOB_KEY: Create_All_Tables
                    LOAD_JOB_KEY: Load_All_Tables
                    TARGET: dev
