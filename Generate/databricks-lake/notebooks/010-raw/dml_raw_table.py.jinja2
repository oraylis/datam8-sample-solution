{% import 'macro/common_python_snippets.py.jinja2' as py_snippets %}

{%- for entity in model.get_raw_entity_list() %}
  {%- set table = entity.model_object.entity %}
  {%- set file_path = helper.build_path("notebooks", SystemProperties().raw_folder, "dml", table.dataProduct, table.dataModule, table.name) %}
>>>>>>>>>> {{ file_path }}.py | py
# Databricks notebook source
# MAGIC %md
# MAGIC # DML for {{entity.model_object.type.value}}.{{table.dataProduct}}_{{table.dataModule}}_{{table.name}}

# COMMAND ----------

# MAGIC %md
# MAGIC # Initialize base settings

# COMMAND ----------

# MAGIC %md
# MAGIC ## Tables Tag
{%- for tag in table.tags %}
# MAGIC - {{tag}}
{%- endfor %}

# COMMAND ----------

# DBTITLE 1,Get variable values

{{ py_snippets.common_variables(entity.model_object.type.value, entity, SystemProperties) }}

# COMMAND ----------

if catalog.is_unity_enabled:
    TARGET_TABLE_PATH = "/Volumes/%(catalog)s/%(zone)s/__files/%(product)s/%(module)s/%(table)s" % {
        "catalog": catalog_name,
        "zone": raw_zone,
        "product": "{{table.dataProduct}}",
        "module": "{{table.dataModule}}",
        "table": "{{table.name}}",
    }
else:
    TARGET_TABLE_PATH = "abfss://%(container)s@%(lake)s.dfs.core.windows.net/%(zone)s/%(product)s/%(module)s/%(table)s" % {  # noqa: E501
        "container": container_name,
        "lake": data_lake_name,
        "zone": raw_zone,
        "product": "{{ table.dataProduct }}",
        "module": "{{ table.dataModule }}",
        "table": "{{ table.name }}",
    }


{% set ns = namespace(delta_column = None) %}
{%- for column in table.attribute%}
  {%- for tag in column.tags if tag == "delta" %}
    {%- set ns.delta_column = column.name%}
  {%- endfor %}
{%- endfor %}
{%- if ns.delta_column %}
# COMMAND ----------

# MAGIC %md
# MAGIC # Retrieve max delta criteria from '{{entity.model_object.type.value}}'

# COMMAND ----------
try:
  dbutils.fs.ls(TARGET_TABLE_PATH)
  spark.read.format("delta").load(TARGET_TABLE_PATH).createOrReplaceTempView("raw_table")
  max_delta = spark.sql("SELECT MAX({{ns.delta_column}}) FROM raw_table").first()[0]
  query_filter = f"WHERE {{ns.delta_column}} > '{max_delta}'"
except Exception as _:
  query_filter = ""
{%- endif %}

{%- set entitySourceLocation = entity.model_object.function.sourceLocation %}
{%- set entityDataSource = entity.model_object.function.dataSource %}
{%- set dataSource = model.data_sources.get_datasource(entityDataSource) %}
  {%- if dataSource.type == 'SqlDataSource' %}
  {% include 'notebooks/010-raw/includes/dml_raw_table_sqldatasource.py.jinja2.include' %}
{%- elif dataSource.type == 'LakeSource' %}
  {% include 'notebooks/010-raw/includes/dml_raw_table_lakesource.py.jinja2.include' %}
{%- endif %}
# COMMAND ----------

# MAGIC %md
# MAGIC # Write to '{{entity.model_object.type.value}}'

# COMMAND ----------

(
    table_df.write.partitionBy("__Year", "__Month", "__Day", "__InsertTimestampUTC")
    .mode("append")
    .format("delta")
    .option("mergeSchema", "true")
    .save(TARGET_TABLE_PATH)
)

# COMMAND ----------

# MAGIC %md
# MAGIC # Update extraction status

# COMMAND ----------

# DBTITLE 1,Update extraction status
count = table_df.count()
print(f"Extracted {count} records in extraction")

<<<<<<<<<< {{ file_path }}.py | py
{%- endfor -%}
