# Macro Import ----------
{% import 'macro/column_declaration_stage.jinja2' as column_declaration %}
{% import 'macro/common_python_snippets.py.jinja2' as py_snippets %}

# Set Gobal Variable for macro input ----------
{%- set payload = Payload(model) %}
{%- set table_payload = payload.get_payload_dml_stage() %}

{%- for entity in table_payload %}
    {%- set column_payload = payload.get_dml_stage_column(entity.locator) %}

>>>>>>>>>> {{ entity.file_path }}.py | py
# Databricks notebook source
# MAGIC %md
# MAGIC # DML for {{ entity.layer_full_table_name }}

# COMMAND ----------

# MAGIC %md
# MAGIC # Initialize base settings

# COMMAND ----------

{{ py_snippets.common_variables(entity.layer_name, entity.table, SystemProperties) }}

# COMMAND ----------

# MAGIC %md
# MAGIC # Read from RAW
# MAGIC ## Source Table tags
{%- for tag in entity.raw_table.tags %}
    # MAGIC - {{ tag }}
{%- endfor %}

# COMMAND ----------

# MAGIC %md
# MAGIC ## Get latest data from '{{ entity.layer_name }}'

# COMMAND ----------

last_partition_df = spark.sql(
    f"""
    select nvl(max(__InsertTimestampUTC), to_timestamp('1970-01-01', 'yyyy-MM-dd')) as LastPartition
    from {stage_zone}.{full_table_name}
"""
)
last_partition = last_partition_df.first()[0]
print(f"Last partition in '{stage_zone}': {last_partition}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Get delta from RAW

# COMMAND ----------

if catalog.is_unity_enabled:
    raw_path = "/Volumes/%(catalog)s/%(zone)s/__files/%(product)s/%(module)s/%(table)s" % {
        "catalog": catalog_name,
        "zone": raw_zone,
        "product": "{{ entity.table.dataProduct }}",
        "module": "{{ entity.table.dataModule }}",
        "table": "{{ entity.table.name }}",
    }
else:
    raw_path = "abfss://%(container)s@%(lake)s.dfs.core.windows.net/%(zone)s/%(product)s/%(module)s/%(table)s" % {  # noqa: E501
        "container": container_name,
        "lake": data_lake_name,
        "zone": raw_zone,
        "product": "{{ entity.table.dataProduct }}",
        "module": "{{ entity.table.dataModule }}",
        "table": "{{ entity.table.name }}",
    }

raw_df = spark.read.format("delta") \
    .load(raw_path) \
    .filter("__InsertTimestampUTC > '%s'" % last_partition)

# COMMAND ----------

# MAGIC %md
# MAGIC # Write to STAGE

# COMMAND ----------

# MAGIC %md
# MAGIC ## Enforce data types
# MAGIC * Enforce data types
# MAGIC * Identify records with non enforcable data types

# COMMAND ----------

raw_df.createOrReplaceTempView("raw_df")

# COMMAND ----------

typed_table_df = spark.sql(
    f"""
SELECT
    -- Table columns
    {{- column_declaration.dml_column_declaration(column_payload) -}}
    -- Technical columns
    CAST(__YEAR AS SMALLINT) AS __Year,
    CAST(__MONTH AS TINYINT) AS __Month,
    CAST(__DAY AS TINYINT) AS __Day,
    CAST(__InsertTimestampUTC AS TIMESTAMP) AS __InsertTimestampUTC
FROM raw_df
"""
)
typed_table_df.createOrReplaceTempView("typed_table_df")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Extract poison records
# MAGIC * Extract poison records
# MAGIC * Write to poison table

# COMMAND ----------

poison_df = spark.sql("""
SELECT
    -- Table columns
{%- for column in entity.table.attribute %}
    `{{ column.name }}`,
{%- endfor %}
    -- Technical columns
    __Year,
    __Month,
    __Day,
    __InsertTimestampUTC
FROM typed_table_df
WHERE
{%- for column in entity.table.attribute %}
    {%- if not loop.first %}
    OR `{{ column.name }}_hasCastError` > 0
    {%- else %}
    `{{ column.name }}_hasCastError` > 0
    {%- endif %}
{%- endfor %}
""")

# COMMAND ----------

poison_df_count = poison_df.count()
if poison_df_count > 0:
    (
        poison_df.write.partitionBy("__Year", "__Month", "__Day", "__InsertTimestampUTC")
        .mode("append")
        .parquet(f"abfss://{container_name}@{data_lake_name}.dfs.core.windows.net/{stage_zone}/{{entity.table.dataProduct}}/{{entity.table.dataModule}}/{{entity.table.name}}_poison")
    )
    print(
        "Extracted %s poison records in extraction" % (poison_df_count)
    )
else:
    print("No poison records present")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Extract valid records
# MAGIC * Extract valid records
# MAGIC * Write to target table

# COMMAND ----------

clean_df = spark.sql("""
SELECT
    -- Table columns
{%- for column in entity.table.attribute %}
    `{{ column.name }}`,
{%- endfor %}
    -- Technical columns
    __Year,
    __Month,
    __Day,
    __InsertTimestampUTC
FROM typed_table_df
WHERE
{%- for column in entity.table.attribute %}
    {%- if not loop.first %}
    AND `{{ column.name }}_hasCastError` == 0
    {%- else %}
    `{{ column.name }}_hasCastError` == 0
    {%- endif %}
{%- endfor %}
""")
clean_df.createOrReplaceTempView("clean_df")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Insert into '{{ entity.layer_full_table_name }}'

# COMMAND ----------

spark.sql("""
INSERT INTO `%(catalog)s`.`%(database)s`.`{{entity.table.dataProduct}}_{{entity.table.dataModule}}_{{ helper.cleanup_name(entity.table.name) }}`
TABLE clean_df
""" % {
  "catalog": catalog.name,
  "database": stage_zone,
})

<<<<<<<<<<
{%- endfor -%}
